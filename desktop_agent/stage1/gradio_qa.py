from modelscope import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
from transformers import BitsAndBytesConfig
import time
import gradio as gr
from PIL import Image
import os
import re
from system_prompt import prompt

if not hasattr(torch.compiler, 'is_compiling'):
    torch.compiler.is_compiling = lambda: False

# æŒ‡å®šæœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„
model_path = "D:\Downloads\Model\Qwen2.5-VL-3B-Instruct"

# 4-bité‡åŒ–é…ç½®
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True
)

print("æ­£åœ¨åŠ è½½æ¨¡å‹...è¯·è€å¿ƒç­‰å¾…...")

# åŠ è½½æ¨¡å‹
model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_path,
    quantization_config=quantization_config,
    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
    device_map="auto",
    local_files_only=True
)

print("æ­£åœ¨åŠ è½½å¤„ç†å™¨...")
# åŠ è½½å¤„ç†å™¨
processor = AutoProcessor.from_pretrained(
    model_path,
    local_files_only=True,
    use_fast=True  # å¼ºåˆ¶ä½¿ç”¨å¿«é€Ÿå¤„ç†å™¨
)


def safe_image_path(path):
    """å®‰å…¨å¤„ç†Windowsè·¯å¾„æ ¼å¼é—®é¢˜"""
    if path.startswith('file=') or '?' in path:
        # å¤„ç†Gradioä¸Šä¼ çš„ä¸´æ—¶æ–‡ä»¶æ ¼å¼
        return re.sub(r"^file=(.*?)(\?.*)?$", r"\1", path)
    return path


def validate_image(image_path):
    """éªŒè¯å›¾ç‰‡è·¯å¾„æ˜¯å¦æœ‰æ•ˆ"""
    if not image_path:
        return False, "è¯·ä¸Šä¼ å›¾ç‰‡"

    clean_path = safe_image_path(image_path)

    if not os.path.exists(clean_path):
        return False, f"å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨: {clean_path}"

    try:
        img = Image.open(clean_path)
        img.verify()  # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆå›¾ç‰‡
        img.close()  # å…³é—­æ–‡ä»¶
        return True, clean_path
    except Exception as e:
        return False, f"æ— æ•ˆå›¾ç‰‡æ–‡ä»¶: {str(e)}"


def generate_response(image_path, question):
    """ç”Ÿæˆæ¨¡å‹å“åº”"""
    # éªŒè¯å¹¶æ¸…ç†å›¾ç‰‡è·¯å¾„
    is_valid, validated_path = validate_image(image_path)
    if not is_valid:
        return validated_path, "0.00ç§’", validated_path

    # å‡†å¤‡æ¶ˆæ¯
    messages = [
        {
            "role": "system",  # ç³»ç»Ÿçº§æŒ‡ä»¤
            "content": prompt
        },
        {
        "role": "user",
        "content": [
            {"type": "image", "image": validated_path},
            {"type": "text", "text": question},
        ],
    }]

    start_time = time.time()

    try:
        # å‡†å¤‡è¾“å…¥
        text = processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, _ = process_vision_info(messages)  # å¿½ç•¥è§†é¢‘è¾“å…¥
        inputs = processor(
            text=[text],
            images=image_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda")

        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=512,
                pad_token_id=processor.tokenizer.pad_token_id
            )

        # è§£ç å“åº”
        decoded_output = processor.batch_decode(
            generated_ids[:, inputs.input_ids.shape[1]:],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )[0]

        # æ¸…ç†è¾“å‡ºæ–‡æœ¬
        output_text = decoded_output.replace("<|im_end|>", "").strip()
        print(output_text)
        elapsed = time.time() - start_time
        return output_text, f"{elapsed:.2f}ç§’", ""
    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        return "", "0.00ç§’", f"æ¨ç†é”™è¯¯: {str(e)}\nè¯¦ç»†ä¿¡æ¯: {tb}"


# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="Qwen2.5-VLå¤šæ¨¡æ€æ¨¡å‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ–¼ï¸ Qwen2.5-VLå¤šæ¨¡æ€æ¨¡å‹æ¼”ç¤º")
    gr.Markdown("ä¸Šä¼ å›¾ç‰‡å¹¶å‘æ¨¡å‹æé—®ï¼Œæ¨¡å‹å°†åˆ†æå›¾ç‰‡å†…å®¹å¹¶å›ç­”æ‚¨çš„é—®é¢˜")

    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(
                type="filepath",
                label="ä¸Šä¼ å›¾ç‰‡",
                height=300,
                interactive=True,
                sources=["upload", "clipboard"]
            )

        with gr.Column(scale=2):
            question_input = gr.Textbox(
                label="æ‚¨çš„é—®é¢˜",
                placeholder="ä¾‹å¦‚ï¼šå›¾ç‰‡ä¸­æœ‰å“ªäº›åº”ç”¨ï¼Ÿå›¾ä¸­æ˜¾ç¤ºäº†ä»€ä¹ˆå†…å®¹ï¼Ÿè¿™äº›å›¾æ ‡çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ",
                lines=3
            )
            with gr.Row():
                submit_btn = gr.Button("æäº¤é—®é¢˜", variant="primary")
                clear_btn = gr.Button("æ¸…ç©ºå†…å®¹", variant="secondary")

    # ç¤ºä¾‹åŒºåŸŸ
    with gr.Accordion("ç‚¹å‡»æŸ¥çœ‹ç¤ºä¾‹é—®é¢˜", open=False):
        with gr.Row():
            gr.Examples(
                examples=[["D:\project-file\PyCharm\desktop_agent\stage1\img.png", "ç½—åˆ—å‡ºå›¾ç‰‡ä¸­æ‰€æœ‰çš„åº”ç”¨"]],
                inputs=[image_input, question_input],
                label="å®Œæ•´ç¤ºä¾‹"
            )

            gr.Examples(
                examples=[
                    ["å›¾ä¸­æœ‰å¤šå°‘ä¸ªå›¾æ ‡ï¼Ÿ"],
                    ["å›¾ç‰‡ä¸­çš„ä¸»è¦åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ"],
                    ["å›¾ç‰‡èƒŒæ™¯æ˜¯ä»€ä¹ˆé¢œè‰²ï¼Ÿ"],
                    ["å›¾ä¸­æœ‰å“ªäº›æ–‡å­—å†…å®¹ï¼Ÿ"],
                    ["ç•Œé¢ä¸­å“ªä¸ªåº”ç”¨æœ€å¼•äººæ³¨ç›®ï¼Ÿ"]
                ],
                inputs=[question_input],
                label="é—®é¢˜ç¤ºä¾‹"
            )

    # ç»“æœåŒºåŸŸ
    with gr.Row():
        output_text = gr.Textbox(
            label="æ¨¡å‹å›ç­”",
            placeholder="ç­‰å¾…å›ç­”...",
            lines=6,
            interactive=False,
            show_copy_button=True
        )

    with gr.Row():
        time_output = gr.Textbox(
            label="æ¨ç†è€—æ—¶",
            interactive=False,
            scale=1
        )
        gr.Textbox(
            label="çŠ¶æ€",
            value="å°±ç»ª",
            interactive=False,
            scale=1
        )
        info_output = gr.Textbox(
            label="ç³»ç»Ÿä¿¡æ¯",
            value=f"PyTorchç‰ˆæœ¬: {torch.__version__}, CUDAå¯ç”¨: {torch.cuda.is_available()}",
            interactive=False,
            scale=2
        )

    error_output = gr.Textbox(
        label="é”™è¯¯ä¿¡æ¯",
        visible=False,
        interactive=False
    )

    # æäº¤å¤„ç†
    submit_btn.click(
        fn=generate_response,
        inputs=[image_input, question_input],
        outputs=[output_text, time_output, error_output]
    )


    # æ¸…ç©ºæŒ‰é’®
    def clear_all():
        return [None, "", "", "0.00ç§’", ""]


    clear_btn.click(
        fn=clear_all,
        outputs=[image_input, question_input, output_text, time_output, error_output]
    )

    # è¾“å…¥å˜åŒ–æ—¶æ¸…ç©ºç»“æœ
    image_input.change(
        fn=lambda: ["", "0.00ç§’", ""],
        outputs=[output_text, time_output, error_output]
    )
    question_input.change(
        fn=lambda: ["", "0.00ç§’", ""],
        outputs=[output_text, time_output, error_output]
    )

# å¯åŠ¨ç•Œé¢
if __name__ == "__main__":
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")

    demo.launch(
        server_name="localhost",
        server_port=7860,
        share=False,
        favicon_path=None,
        show_error=True
    )